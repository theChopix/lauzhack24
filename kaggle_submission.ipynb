{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "import numpy as np\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "import sys\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "sys.setrecursionlimit(1000000)\n",
    "\n",
    "# Load CSV file\n",
    "external_parties = pd.read_csv('data/external_parties_train.csv')\n",
    "\n",
    "# Clean the DataFrame\n",
    "#-----------------------\n",
    "\n",
    "\n",
    "# Make sure you have the necessary NLTK resources\n",
    "nltk.download('stopwords', force=True)\n",
    "nltk.download('punkt', force=True)\n",
    "nltk.download('punkt_tab', force=True)\n",
    "\n",
    "# Initialize stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Process the text\n",
    "external_parties['processed_text'] = (\n",
    "    external_parties['party_info_unstructured']\n",
    "    .fillna('')  # Handle NaN values\n",
    "    .astype(str)  # Ensure string type\n",
    "    .str.lower()  # Convert to lowercase\n",
    "    .apply(lambda x: ' '.join([word for word in word_tokenize(x) if word not in stop_words]))  # Remove stopwords\n",
    ")\n",
    "\n",
    "print(external_parties['processed_text'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSHEntityResolution:\n",
    "    def __init__(self, dataframe):\n",
    "        \"\"\"\n",
    "        Initialize with the DataFrame.\n",
    "        :param dataframe: Pandas DataFrame containing entity data.\n",
    "        \"\"\"\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    # Generate MinHash for a given text\n",
    "    def generate_minhash(self, text, num_perm=128):\n",
    "        \"\"\"\n",
    "        Create MinHash for a given text.\n",
    "        :param text: Input string.\n",
    "        :param num_perm: Number of permutations for MinHash.\n",
    "        :return: MinHash object.\n",
    "        \"\"\"\n",
    "        if not text or pd.isna(text):\n",
    "            return None\n",
    "        mh = MinHash(num_perm=num_perm)\n",
    "        for token in text.split():\n",
    "            mh.update(token.encode(\"utf8\"))\n",
    "        return mh\n",
    "\n",
    "    # Perform LSH on nodes\n",
    "    def perform_lsh(self, threshold=0.8, num_perm=128):\n",
    "        \"\"\"\n",
    "        Perform LSH to find similar entities.\n",
    "        :param threshold: Similarity threshold for LSH.\n",
    "        :param num_perm: Number of permutations for MinHash.\n",
    "        :return: List of matched entity pairs.\n",
    "        \"\"\"\n",
    "        lsh = MinHashLSH(threshold=threshold, num_perm=num_perm)\n",
    "        node_hashes = {}\n",
    "\n",
    "        # Generate combined text and MinHashes in a vectorized way\n",
    "        combined_texts = self.dataframe['processed_text'].fillna('').astype(str)\n",
    "\n",
    "        # Initialize tqdm progress bar for MinHash generation\n",
    "        minhashes = []\n",
    "        for text in tqdm(combined_texts, desc=\"Generating MinHashes\", total=len(combined_texts)):\n",
    "            mh = self.generate_minhash(text, num_perm)\n",
    "            minhashes.append(mh)\n",
    "\n",
    "        # Convert the list of minhashes into a pandas Series to handle it further\n",
    "        minhashes = pd.Series(minhashes)\n",
    "\n",
    "        # Filter out None values (in case of empty or invalid text)\n",
    "        valid_minhashes = minhashes.dropna()\n",
    "\n",
    "        # Insert nodes into LSH using the valid MinHashes\n",
    "        for index, mh in tqdm(valid_minhashes.items(), desc=\"Inserting into LSH\", total=len(valid_minhashes)):\n",
    "            lsh.insert(str(index), mh)\n",
    "            node_hashes[str(index)] = mh\n",
    "\n",
    "        # Find similar pairs\n",
    "        matches = []\n",
    "        for node_id, mh in tqdm(node_hashes.items(), desc=\"Finding matches\", total=len(node_hashes)):\n",
    "            similar = lsh.query(mh)\n",
    "            for sim in similar:\n",
    "                if int(node_id) < int(sim):  # Avoid duplicate matches (e.g., (A, B) and (B, A))\n",
    "                    matches.append((int(node_id), int(sim)))\n",
    "\n",
    "        return matches\n",
    "\n",
    "\n",
    "# Instantiate the LSHEntityResolution class\n",
    "lsh_er = LSHEntityResolution(external_parties)\n",
    "\n",
    "# Perform LSH to find matches\n",
    "matches = lsh_er.perform_lsh(threshold=0.5, num_perm=254)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph:\n",
    "    # init function to declare class variables\n",
    "    def __init__(self, V):\n",
    "        self.V = V\n",
    "        self.adj = {}\n",
    "        for v in V:\n",
    "            self.adj[v] = []\n",
    " \n",
    "    def DFSUtil(self, temp, v, visited):\n",
    " \n",
    "        # Mark the current vertex as visited\n",
    "        visited[v] = True\n",
    " \n",
    "        # Store the vertex to list\n",
    "        temp.append(v)\n",
    " \n",
    "        # Repeat for all vertices adjacent\n",
    "        # to this vertex v\n",
    "        for i in self.adj[v]:\n",
    "            if visited[i] == False:\n",
    " \n",
    "                # Update the list\n",
    "                temp = self.DFSUtil(temp, i, visited)\n",
    "        return temp\n",
    " \n",
    "    # method to add an undirected edge\n",
    "    def addEdge(self, v, w):\n",
    "        self.adj[v].append(w)\n",
    "        self.adj[w].append(v)\n",
    " \n",
    "    # Method to retrieve connected components\n",
    "    # in an undirected graph\n",
    "    def connectedComponents(self):\n",
    "        visited = {}\n",
    "        cc = []\n",
    "        for i in self.V:\n",
    "            visited[i] = False\n",
    "        for v in self.V:\n",
    "            if visited[v] == False:\n",
    "                temp = []\n",
    "                cc.append(self.DFSUtil(temp, v, visited))\n",
    "        return cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "g = Graph(np.arange(len(external_parties)))\n",
    "\n",
    "for idx, (a,b) in tqdm(enumerate(matches)):\n",
    "    g.addEdge(a,b)\n",
    "\n",
    "c = g.connectedComponents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_id = np.arange(30000,30000+len(external_parties))\n",
    "\n",
    "for idx, c_i in enumerate(c):\n",
    "    for c_j in c_i:\n",
    "        pred_id[c_j] = pred_id[c_i[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_datasets(train, test):\n",
    "\n",
    "    def create_pairwise_matrix(external_parties_df):\n",
    "        \"\"\"creates matrix that tells us if two parties are the same identity\"\"\"\n",
    "        external_ids = external_parties_df['external_id'].values\n",
    "        matrix = (external_ids[:, None] == external_ids).tolist()\n",
    "        return matrix\n",
    "    \n",
    "    def compute_recall(matrix_truth, matrix_pred):\n",
    "        n = len(matrix_truth)\n",
    "        true_positive = 0\n",
    "        false_negative = 0\n",
    "        for i in range(n):\n",
    "            for j in range(i + 1, n):\n",
    "                if matrix_truth[i][j] and matrix_pred[i][j]:\n",
    "                    true_positive += 1\n",
    "                elif matrix_truth[i][j] and not matrix_pred[i][j]:\n",
    "                    false_negative += 1\n",
    "        recall = true_positive / (true_positive + false_negative) if (true_positive + false_negative) > 0 else 0\n",
    "        return recall\n",
    "\n",
    "    def compute_precision(matrix_truth, matrix_pred):\n",
    "        n = len(matrix_truth)\n",
    "        true_positive = 0\n",
    "        false_positive = 0\n",
    "        for i in range(n):\n",
    "            for j in range(i + 1, n):\n",
    "                if matrix_truth[i][j] and matrix_pred[i][j]:\n",
    "                    true_positive += 1\n",
    "                elif not matrix_truth[i][j] and matrix_pred[i][j]:\n",
    "                    false_positive += 1\n",
    "        precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0\n",
    "        return precision\n",
    "\n",
    "\n",
    "    train_matrix = create_pairwise_matrix(train)\n",
    "    test_matrix  = create_pairwise_matrix(test)\n",
    "\n",
    "    recall = compute_recall(train_matrix, test_matrix)\n",
    "    precision = compute_precision(train_matrix, test_matrix)\n",
    "\n",
    "    f1 = 2 * recall * precision / (recall + precision) if (recall + precision) > 0 else 0\n",
    "    return f1\n",
    "\n",
    "dict_ = {'transaction_reference_id': external_parties['transaction_reference_id'], 'external_id':pred_id}\n",
    "print(dict_)\n",
    "pred_dict = pd.DataFrame.from_dict(dict_)\n",
    "\n",
    "print(evaluate_datasets(external_parties,pred_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(dict_).to_csv('submission_4.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
