{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\chris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\chris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\chris\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          therese humphrey 6223 john brokos wilvoxside 7...\n",
      "1          holly dudley 1151 doug las islands danniellesi...\n",
      "2          mcfarlan dknc 74099 joseph inlet ramirezport 7...\n",
      "3          jamie cooper 50019 austin light johnbury saudi...\n",
      "4          fernandez , frost gonzalez 6776 thomas crescen...\n",
      "                                 ...                        \n",
      "1481667              allen-gonzalez west melissa 75288 tonga\n",
      "1481668    mccarthh llc ( malvinas ) 11926 alvarado ranch...\n",
      "1481669    garcia-moore 2742 johnson bridge terrimouth la...\n",
      "1481670    santiago-oconnor 748 chang fields williamstad ...\n",
      "1481671    cortez-chan co. 42035 beth ln . david liechten...\n",
      "Name: processed_text, Length: 1481672, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "import numpy as np\n",
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "import sys\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "sys.setrecursionlimit(1000000)\n",
    "\n",
    "# Load CSV file\n",
    "external_parties = pd.read_csv('data/external_parties_test.csv')\n",
    "\n",
    "# Clean the DataFrame\n",
    "#-----------------------\n",
    "\n",
    "\n",
    "# Make sure you have the necessary NLTK resources\n",
    "nltk.download('stopwords', force=True)\n",
    "nltk.download('punkt', force=True)\n",
    "nltk.download('punkt_tab', force=True)\n",
    "\n",
    "# Initialize stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Process the text\n",
    "external_parties['processed_text'] = (\n",
    "    external_parties['party_info_unstructured']\n",
    "    .fillna('')  # Handle NaN values\n",
    "    .astype(str)  # Ensure string type\n",
    "    .str.lower()  # Convert to lowercase\n",
    "    .apply(lambda x: ' '.join([word for word in word_tokenize(x) if word not in stop_words]))  # Remove stopwords\n",
    ")\n",
    "\n",
    "print(external_parties['processed_text'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating MinHashes: 100%|██████████| 1481672/1481672 [45:46<00:00, 539.48it/s] \n",
      "Inserting into LSH: 100%|██████████| 1481672/1481672 [03:41<00:00, 6696.96it/s] \n",
      "Finding matches: 100%|██████████| 1481672/1481672 [03:31<00:00, 7005.78it/s] \n"
     ]
    }
   ],
   "source": [
    "class LSHEntityResolution:\n",
    "    def __init__(self, dataframe):\n",
    "        \"\"\"\n",
    "        Initialize with the DataFrame.\n",
    "        :param dataframe: Pandas DataFrame containing entity data.\n",
    "        \"\"\"\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "    # Generate MinHash for a given text\n",
    "    def generate_minhash(self, text, num_perm=128):\n",
    "        \"\"\"\n",
    "        Create MinHash for a given text.\n",
    "        :param text: Input string.\n",
    "        :param num_perm: Number of permutations for MinHash.\n",
    "        :return: MinHash object.\n",
    "        \"\"\"\n",
    "        if not text or pd.isna(text):\n",
    "            return None\n",
    "        mh = MinHash(num_perm=num_perm)\n",
    "        for token in text.split():\n",
    "            mh.update(token.encode(\"utf8\"))\n",
    "        return mh\n",
    "\n",
    "    # Perform LSH on nodes\n",
    "    def perform_lsh(self, threshold=0.8, num_perm=128):\n",
    "        \"\"\"\n",
    "        Perform LSH to find similar entities.\n",
    "        :param threshold: Similarity threshold for LSH.\n",
    "        :param num_perm: Number of permutations for MinHash.\n",
    "        :return: List of matched entity pairs.\n",
    "        \"\"\"\n",
    "        lsh = MinHashLSH(threshold=threshold, num_perm=num_perm)\n",
    "        node_hashes = {}\n",
    "\n",
    "        # Generate combined text and MinHashes in a vectorized way\n",
    "        combined_texts = self.dataframe['processed_text'].fillna('').astype(str)\n",
    "\n",
    "        # Initialize tqdm progress bar for MinHash generation\n",
    "        minhashes = []\n",
    "        for text in tqdm(combined_texts, desc=\"Generating MinHashes\", total=len(combined_texts)):\n",
    "            mh = self.generate_minhash(text, num_perm)\n",
    "            minhashes.append(mh)\n",
    "\n",
    "        # Convert the list of minhashes into a pandas Series to handle it further\n",
    "        minhashes = pd.Series(minhashes)\n",
    "\n",
    "        # Filter out None values (in case of empty or invalid text)\n",
    "        valid_minhashes = minhashes.dropna()\n",
    "\n",
    "        # Insert nodes into LSH using the valid MinHashes\n",
    "        for index, mh in tqdm(valid_minhashes.items(), desc=\"Inserting into LSH\", total=len(valid_minhashes)):\n",
    "            lsh.insert(str(index), mh)\n",
    "            node_hashes[str(index)] = mh\n",
    "\n",
    "        # Find similar pairs\n",
    "        matches = []\n",
    "        for node_id, mh in tqdm(node_hashes.items(), desc=\"Finding matches\", total=len(node_hashes)):\n",
    "            similar = lsh.query(mh)\n",
    "            for sim in similar:\n",
    "                if int(node_id) < int(sim):  # Avoid duplicate matches (e.g., (A, B) and (B, A))\n",
    "                    matches.append((int(node_id), int(sim)))\n",
    "\n",
    "        return matches\n",
    "\n",
    "\n",
    "# Instantiate the LSHEntityResolution class\n",
    "lsh_er = LSHEntityResolution(external_parties)\n",
    "\n",
    "# Perform LSH to find matches\n",
    "matches = lsh_er.perform_lsh(threshold=0.5, num_perm=254)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Graph:\n",
    "    # init function to declare class variables\n",
    "    def __init__(self, V):\n",
    "        self.V = V\n",
    "        self.adj = {}\n",
    "        for v in V:\n",
    "            self.adj[v] = []\n",
    " \n",
    "    def DFSUtil(self, temp, v, visited):\n",
    " \n",
    "        # Mark the current vertex as visited\n",
    "        visited[v] = True\n",
    " \n",
    "        # Store the vertex to list\n",
    "        temp.append(v)\n",
    " \n",
    "        # Repeat for all vertices adjacent\n",
    "        # to this vertex v\n",
    "        for i in self.adj[v]:\n",
    "            if visited[i] == False:\n",
    " \n",
    "                # Update the list\n",
    "                temp = self.DFSUtil(temp, i, visited)\n",
    "        return temp\n",
    " \n",
    "    # method to add an undirected edge\n",
    "    def addEdge(self, v, w):\n",
    "        self.adj[v].append(w)\n",
    "        self.adj[w].append(v)\n",
    " \n",
    "    # Method to retrieve connected components\n",
    "    # in an undirected graph\n",
    "    def connectedComponents(self):\n",
    "        visited = {}\n",
    "        cc = []\n",
    "        for i in self.V:\n",
    "            visited[i] = False\n",
    "        for v in self.V:\n",
    "            if visited[v] == False:\n",
    "                temp = []\n",
    "                cc.append(self.DFSUtil(temp, v, visited))\n",
    "        return cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a0882f040b4437282a621a610daa73b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "g = Graph(np.arange(len(external_parties)))\n",
    "\n",
    "for idx, (a,b) in tqdm(enumerate(matches)):\n",
    "    g.addEdge(a,b)\n",
    "\n",
    "c = g.connectedComponents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_id = np.arange(30000,30000+len(external_parties))\n",
    "\n",
    "for idx, c_i in enumerate(c):\n",
    "    for c_j in c_i:\n",
    "        pred_id[c_j] = pred_id[c_i[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  30000,   30001,   30002, ..., 1511669, 1511670, 1511671])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'transaction_reference_id': 0          8f71ed0a819236b141978defe9a98700\n",
      "1          dc9be336b81b971c04ca98ccdf99d51e\n",
      "2          29c8ed895b8a220121168158a7447773\n",
      "3          38ee5d04ad50ddf5f6abc67017201548\n",
      "4          eef4f720e71bda0a6f1e7caa21a2c98c\n",
      "                         ...               \n",
      "1481667    a3b0502a9917c0da0c0343055d434c8f\n",
      "1481668    38aac541d4b2cef7c05b584abfcca494\n",
      "1481669    28f3dfc3160bae3b92c7c42e1082cf43\n",
      "1481670    39f8569b0ced6d11fad95d628dd05d6a\n",
      "1481671    ca917bf58677e7cd34b21d38a19ee17c\n",
      "Name: transaction_reference_id, Length: 1481672, dtype: object, 'external_id': array([  30000,   30001,   30002, ..., 1511669, 1511670, 1511671])}\n"
     ]
    }
   ],
   "source": [
    "def evaluate_datasets(train, test):\n",
    "\n",
    "    def create_pairwise_matrix(external_parties_df):\n",
    "        \"\"\"creates matrix that tells us if two parties are the same identity\"\"\"\n",
    "        external_ids = external_parties_df['external_id'].values\n",
    "        matrix = (external_ids[:, None] == external_ids).tolist()\n",
    "        return matrix\n",
    "    \n",
    "    def compute_recall(matrix_truth, matrix_pred):\n",
    "        n = len(matrix_truth)\n",
    "        true_positive = 0\n",
    "        false_negative = 0\n",
    "        for i in range(n):\n",
    "            for j in range(i + 1, n):\n",
    "                if matrix_truth[i][j] and matrix_pred[i][j]:\n",
    "                    true_positive += 1\n",
    "                elif matrix_truth[i][j] and not matrix_pred[i][j]:\n",
    "                    false_negative += 1\n",
    "        recall = true_positive / (true_positive + false_negative) if (true_positive + false_negative) > 0 else 0\n",
    "        return recall\n",
    "\n",
    "    def compute_precision(matrix_truth, matrix_pred):\n",
    "        n = len(matrix_truth)\n",
    "        true_positive = 0\n",
    "        false_positive = 0\n",
    "        for i in range(n):\n",
    "            for j in range(i + 1, n):\n",
    "                if matrix_truth[i][j] and matrix_pred[i][j]:\n",
    "                    true_positive += 1\n",
    "                elif not matrix_truth[i][j] and matrix_pred[i][j]:\n",
    "                    false_positive += 1\n",
    "        precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0\n",
    "        return precision\n",
    "\n",
    "\n",
    "    train_matrix = create_pairwise_matrix(train)\n",
    "    test_matrix  = create_pairwise_matrix(test)\n",
    "\n",
    "    recall = compute_recall(train_matrix, test_matrix)\n",
    "    precision = compute_precision(train_matrix, test_matrix)\n",
    "\n",
    "    f1 = 2 * recall * precision / (recall + precision) if (recall + precision) > 0 else 0\n",
    "    return f1\n",
    "\n",
    "dict_ = {'transaction_reference_id': external_parties['transaction_reference_id'], 'external_id':pred_id}\n",
    "print(dict_)\n",
    "#pred_dict = pd.DataFrame.from_dict(dict_)\n",
    "\n",
    "#print(evaluate_datasets(external_parties,pred_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(dict_).to_csv('submission_1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_dict(dict_)\n",
    "# Count occurrences of each external_id\n",
    "id_counts = df['external_id'].value_counts()\n",
    "\n",
    "# Filter the DataFrame to keep rows where external_id appears more than once\n",
    "filtered_df = df[df['external_id'].isin(id_counts[id_counts > 1].index)]\n",
    "filtered_df.to_csv('submission_2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.to_csv('submission_1.csv.gz', index=False, compression='gzip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "694744"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
